# **機器學習概論**
* 傳統演算法vs資料驅動演算法
  並非直接告訴電腦人臉的規則，而是電腦直接看過一大堆例子後自己歸納判斷規則

* 機器學習的分類
    1. 監督式學習
       在學習的過程中有人監督你，直接告訴你這個答案是什麼，學習過程中有問題和答案
       ex.桌子是desk
        * 分類問題：猜測是多選一的類別 
        * 回歸問題：猜測是某一個的數字

    2. 非監督式學習
       小朋友在學母語的時候，多聽爸媽說自然學會的
    3. 半監督式學習
    4. 強化學習

* 監督式學習的演算方法
    1. K-nearest neibors classifior
        * 在預測值的附近設定K個鄰居，判斷中k裡面哪種類別的值最多 
        * 如何選擇K(超參數)的數字?
        * overfit(k越小) vs underfit(k越大)
    2. 決策樹分類器
        * 不需要把訓練資料留下來，會留下一個有規則的決策數
        * 透過商(亂度)的降低多寡來選擇分類
        * 當某個子節點的數量太少的時候就停止分類，避免overfit
    3. 隨機森林
        * 每一棵樹只能隨機看到一部份的feature
        * 每一棵樹只能隨機看到一部份的sample
        * 透過這樣的方式產生好幾棵樹之後，將預測資料帶入所有的樹，選擇預測結果最多的那個。
        *  deeplearning 很適合用在當你的某些類別的意義不明的時候，可以組合出有意義的feature，但是當你的類別意義明確的時候，隨機森林的效果就會很不錯。

* 線性回歸
    1. deeplearning在最小化residual的方法：
        * 先給一個初始值，透過一個方法去細小的調整參數的值，使得residual的數字最小
    2. 什麼時後會overfit:
        * 當你估計的參數數字正(或負)很大的時候
        * 在最小化residual的同時，加入所以參數的平方和(regulization)也要最小化的條件，可以有效避免overfit
    3. 什麼時候要用Ridge：可以接受某個類別的係數很小
    4. 什麼時候用Lasso：可以接受某個類別的係數為0

* 邏吉斯回歸
    1. 可以用S型的function來fit二元的資料
    2. loss function ：cross entropy loss(不用MSE是因為0和1的距離太小懲罰性不足)

    

  
  
