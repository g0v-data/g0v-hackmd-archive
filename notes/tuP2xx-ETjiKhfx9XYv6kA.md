---
tags: vTaiwan, openAI
---

# 9/24 English Transcript (1/2) of Citizen Assembly on Demoratic Input of AI


:::info

- [9/24 Assembly meeting note](/aHOvZGQyRlamN8Qogp9guw)
- [Youtube recording](https://www.youtube.com/watch?v=quOjUG6X8w4)

:::

[07:21 - 07:44]

**Peter:**
Later, we will utilize some online tools to help everyone, in addition to the on-site discussions, also interact online. Meanwhile, friends have been joining us online, including Zheng Xiang from the Judicial Reform Foundation, as well as Sanya and Wendy. If we need to use the internet, for Wi-Fi, we can use "Meeting Guest". Later, we will utilize some online tools to help everyone, in addition to the on-site discussions, also interact online. Meanwhile, friends have been joining us online, including Zheng Xiang from the Judicial Reform Foundation, as well as Sanya and Wendy. If we need to use the internet, for Wi-Fi, we can use "Meeting Guest".

[07:44 - 08:06]

**Peter:**
For this Wi-Fi "Meeting Guest", we will use Sli.do to collect everyone's questions and answers because during the Q&A time later on, each person's speaking time on-site may be limited to ensure the speaking rights of every participant. So, if you have any questions or if you haven't had the chance to fully express your answers, you're welcome to use Sli.do.

[08:06 - 08:29]

**Peter:**
We will collect all on there, and all discussions and records will be archived and will be presented in our report. Meanwhile, the large projected QR Code above is the QR Code for Sli.do, which you can scan directly. The smaller QR Code is for continued participation in VTaiwan. If you want to continue participating in VTaiwan, you can also scan the QR Code above. The smaller QR Code for continued participation in VTaiwan and the QR Code on the poster are the same.

[08:29 - 08:53]

**Peter:**
Welcome everyone. Alright, without further ado, since it's about time, let's officially start the event. Hello everyone, welcome to the VTaiwan Human Rights Consultation Meeting. I am the host Peter, and the other host is Erica. Erica, would you like to introduce yourself?

**Erica:**
Hello everyone, I am the other host.
[08:53 - 09:15]

**Erica:**
My name is Erica. Hi, everyone.

**Peter:**
We are the two hosts for today's event, responsible for guiding and leading everyone through today's consultation meeting. The agenda for today will be roughly as displayed on the slides. Our current schedule has been slightly delayed by about five minutes.

[09:15 - 09:39]

**Peter:**
If we follow the established agenda, I will first give an introduction to the event plan and what VTaiwan is, an introduction to the VTaiwan process. Then, there will be 15 minutes for both on-site and online participants to introduce themselves. The self-introduction has a fixed format, so everyone may need to follow that format to avoid affecting the speaking time of others. Following that, there will be a 15-minute background report.

[09:39 - 10:02]

**Peter:**
This includes addressing the overall background of Artificial Intelligence and ethical guidelines, as well as the feedback gathered so far through our opinion collection, which will be reported. Following this, we will discuss Topic 1, Topic 2, and Topic 3 respectively. In the end, there will be a summary by the host and a final round of opinion collection and presentation. So, if anyone has any opinions on Topic 1, Topic 2, or Topic 3 that haven't been shared or weren’t able to be shared,
[10:02 - 10:25]

**Peter:**
you can supplement them in the final stage or make good use of our online tools, whether it’s the collaborative link that has already been placed in the email. If interested, you can go directly to the email sent to everyone, it’s all there. Or you can go to Slido to post your questions or share your discussions and opinions. Meanwhile, our online audience can also follow us on Facebook and YouTube.

[10:25 - 10:47]

**Peter:**
We have live streams, so the live audience, we have arranged agenda assistants on-site to help post related opinions in the chat room. So, everyone is welcome to express their opinions in various ways. Well, without further delay, let me start by—oh, the poster fell down—without further delay, let me start with the introduction to the event and the plan. Okay, first of all, the introduction to the event and the plan is divided into three parts.

[10:47 - 11:09]

**Peter:**
Firstly, I want to introduce what vTaiwan is, followed by what Bridge in the Recursive Public is, which is also why we are here today and why we are able to hold this consultation meeting. Lastly, what we want to introduce to everyone is how our vTaiwan goes through the whole process. Firstly, what is vTaiwan, you might be curious about the origins of vTaiwan, what the process of vTaiwan really is, but actually its origin is something

[11:10 - 11:32]

**Peter:**
that everyone should be quite familiar with. Actually, it was during the Sunflower Student Movement in 2014. The Sunflower Student Movement changed the entire history of social movements in Taiwan, and at the same time, it ignited a lot of civic tech and civic energy. The movement's occurrence has made the public realize that society began to pulse and change, but for the government, they realized

[11:32 - 11:55]

**Peter:**
that the traditional opinion collection process seemed inadequate. Hence, at that time, the Minister without Portfolio, Tsai Yu-ling, came to the g0v civic tech community with a proposal, hoping to establish a new digital legal adaptation platform. And this adaptation platform is precisely the birth of vTaiwan. Now, what's different about vTaiwan compared to previous law and policy-making processes?

[11:55 - 12:18]

**Peter:**
Before the emergence of vTaiwan, law-making often was led by experts, including some scholars. However, these scholars, generally speaking, if you were more of an opposing opinion, it might not be easy to get involved in the government as a project designer. Meanwhile, there would also be influences from corporate lobbying. The most important issue is that the entire discussion process was in a quite opaque state. Relevant meeting records, even if they were public, were in a quite incomplete state.

[12:18 - 12:41]

**Peter:**
With vTaiwan, we aim to create a community-oriented platform, hoping that the community takes charge of the entire legislative drafting process, connecting different stakeholders. Throughout the process, whether it's the initial opinion collection using Polis, or the ongoing consultation meetings, all will be broadcasted live online.

[12:41 - 13:03]

**Peter:**
Through online collaborative notes or live streaming, the process of opinion collection, linking, and discussion is made public and transparent. This roughly outlines the process of vTaiwan, where stakeholders bring their concerns into the process, initially shaping opinions which then get refined through community discussions while also collecting outside opinions.

[13:03 - 13:25]

**Peter:**
These discussions and collections will keep converging, and eventually, a draft that can respond to initial opinions will be formed. This draft will then become part of the regulations. One unique feature of participating in vTaiwan compared to previous community movements or organizations is that everyone had to play a specific role and dedicate a lot of effort.

[13:25 - 13:47]

**Peter:**
But within vTaiwan, we have different levels of participation, like "Pit Masters," heavy, moderate, light, or even nano-level participants, who can all contribute at various capacities. So, you don't need to worry if you don’t have enough time to participate in the entire process. Everyone, at different points, can play different roles.

[13:47 - 14:09]

**Peter:**
Everyone can play a crucial role in the whole vTaiwan process. This is what we hope to achieve through community participation in vTaiwan. Okay, next, I will introduce what Bridging the Recursive Public is. Bridging the Recursive Public originates from a project launched by OpenAI this year called Democratic Input to AI. We all know that after OpenAI launched tools like

[14:09 - 14:32]

**Peter:**
Chat GPT, and DALL-E 1, DALL-E 2, DALL-E 3, the impacts and waves created by generative AI have begun affecting business operations, societal norms, and even caused disruptions in society and regulations. Recognizing this, OpenAI launched the Democratic Input to AI project.

[14:32 - 14:54]

**Peter:**
They aim to find ten different teams worldwide, hoping to harness technology or community power to find a way to democratize AI further. Excitingly, vTaiwan, in collaboration with a renowned UK think tank called Chatham House, applied for this project and successfully stood out among eight hundred teams, becoming one of the final

[14:54 - 15:17]

**Peter:**
ten selected teams. This is a significant milestone as we are the only team with Taiwanese participation. So, it’s a project where we can include our opinions, including everyone here today, in our report. All discussions today will be included in our report, which will be seen by OpenAI’s

[15:17 - 15:40]

**Peter:**
development team, their internal PR team, and even the management. Our goal is to use the vTaiwan process to incorporate more diverse opinions into the development and deployment of AI, from various stakeholders. This aligns with the US National Institute of Standards and Technology (NIST) who previously published an

[15:40 - 16:04]

**Peter:**
AI Risk Management framework. In this framework, they outlined four major elements of risk management, one of the most crucial being the governance aspect which sets directions on how to avoid risks during AI development. In this governance segment,

[16:04 - 16:49]

**Peter:**
The most important aspect is to incorporate diverse stakeholder opinions, allowing not only individuals with technical backgrounds but also those from different societal backgrounds with existing domain knowledge to join this process. Moving on, there may be some criticisms, such as why OpenAI is doing this. Are they engaging in something called "open washing," akin to "green washing," a term often heard to describe a superficial or deceitful display? This criticism indeed exists, and it's something we in the vTaiwan project, whether on the Taiwan side or the UK side with Chatham House, have been striving to avoid.

[16:49 - 17:11]

**Peter:**
How to avoid it? Firstly, we hope to involve OpenAI in the subsequent discussions as much as possible. That's why we invited representatives from OpenAI to today's meeting, although we were unable to get them to attend. However, they did participate in the roundtable meetings in the UK. So, we aim to involve OpenAI in subsequent discussions and make the relevant data public.

[17:11 - 17:35]

**Peter:**
By doing so, future researchers can refer to these processes when researching the formulation of regulations or AI ethical norms, which we hope can help mitigate criticisms of "open washing" or putting up a show. Meanwhile, our community will continue to have related discussions.

[17:35 - 17:59]

**Peter:**
As for the vTaiwan process, as you can see from the slides, it's divided into five different stages: setting core issues, opinion collection, the consultation meetings (which is where we are now), data organization and consolidation, and reporting. Firstly, regarding setting the core issues, when we were vying for the OpenAI's Democratic Input to AI Grant,

[17:59 - 18:22]

**Peter:**
we were allowed to choose our topic. We chose this: when AI is dealing with human rights or local legal and cultural differences, such as LGBTQ or women's issues, what principles should AI follow? Should AI alter its responses based on cultural and regional differences during use? This is the topic we chose for discussion.

[18:22 - 18:44]

**Peter:**
So, we need to brainstorm how to facilitate subsequent discussions from this topic. But as you may notice, one significant challenge is that it's a very abstract topic with a broad scope of discussion and not many concrete cases. To continue the discussion, we must make it more concrete.

[18:45 - 19:07]

**Peter:**
We aim to make the discussion more concrete through various means, like gathering opinions during our weekly meetings, and inviting researchers like Dr. Mei-Jun Li from Academia Sinica or other experts from social sciences and legal backgrounds for interviews. We also chat with participants from different backgrounds.

[19:07 - 19:29]

**Peter:**
Then we collect and consolidate the information, resulting in a set of pre-reading materials that you might have received before this event. The content of these materials was derived from our discussions and forms the framework for today's discussion. Now, with this framework, we can proceed with the opinion collection phase, which is our opinion collection process.

[19:29 - 19:52]

**Peter:**
Our objective is to achieve a balance between framework and openness. Hence, we threw 30 different seed opinions onto POLIS and encouraged everyone to participate in this POLIS opinion collection. We were thrilled to see that in the end, 188 individuals were willing to participate, sharing their views.

[19:52 - 20:15]

**Peter:**
There were over 4,500 votes cast, and aside from the initial 30 seed opinions, 45 new opinions were generated. This made us very happy as it proved that our discussion indeed helped in building a preliminary framework for AI ethical norms and provided a platform for everyone to share their thoughts and viewpoints.

[20:15 - 20:38]

**Peter:**
Now, onto the consultation meeting segment, where you all are. After organizing the topics from POLIS, we found that the opinions, including the seed opinions, majorly revolved around three areas: Data and Privacy Protection, Discrimination and Bias possibly brought by AI, and Localization. We will use these three areas as the framework for our discussion shortly.

[20:38 - 21:02]

**Peter:**
We have invited many incredible stakeholders from various fields, and I am very thankful for everyone's participation in today's consultation meeting. The original design of vTaiwan included a responder role, but we are still tweaking this, exploring whether we should have OpenAI or other AI developing companies take on this responder role. We will work on this in subsequent procedures.

[21:02 - 21:24]

**Peter:**
Next will be data organization, consolidation, and finally, the report. On October 20, we will present a report regarding the entire vTaiwan process and experiences to OpenAI. Next week, we are heading to San Francisco to visit OpenAI’s headquarters, chat with the OpenAI team, and hope to bring the voice of Taiwan into Chat GPT.

[21:24 - 21:46]

**Peter:**
That’s the primary objective of gathering here and participating in this consultation meeting today. Thank you all. Now, we have some administrative announcements. During the discussion, please avoid offensive language, respect those with differing opinions, and be mindful of your speaking time.

[21:46 - 22:10]

**Peter:**
Our moderators will kindly remind you to wrap up. If you have more to say, please use the shared document or Slido link for commenting. Some administrative details include the location of drinking water and restrooms on the first and second floors. Eating is not allowed in this venue. If you have any questions, feel free to ask any staff member with a name tag like this.

[22:10 - 22:33]

**Peter:**
Now, it's time for self-introductions. Given the number of attendees, to save time, we hope everyone can follow a specific format for self-introduction. The hosts will demonstrate. Please state who you are, where you are from, and what you hope AI can achieve.

[22:34 - 22:56]

**Peter:**
For example, I am Tsui Chiawei (my Chinese name). I am from the G0V temporary government community. I hope the development of AI can incorporate more diverse opinions. Without further ado, let’s have the host on stage, the second host, to introduce themselves following this format. Over to you, Erica.

[22:56 - 23:22]

**Erica:**
Thanks, Peter. I am Erica from the design community, and I hope the development of AI can be inclusive, allowing individuals of all races, genders, and types to participate. Now, shall we start from the left or right, maybe the first row? Over to RR.

**RR:**
Hello, I am RR.

[23:22 - 23:47]

**RR:**
I come from the G0V community and currently work in the digital development department. I hope the development of AI can bring more innovation and creativity to government departments.

**Zhou Yicheng:**
Hello, I am Zhou Yicheng from the entrepreneurial and Wild Lily Generation community.

[23:47 - 24:09]

**Zhou Yicheng:**
I hope AI can promote social equality and the development of democracy. Over to Nicole.

**Nicole:**
Hello, I am Zhān Tíng Yí, Nicole. I am... well, let's say, I am lawyer Zhān. I hope AI can...

[24:09 - 24:33]

**Nicole:**
...create a better life for everyone and allow more people to express their opinions freely and fully.

**Hong Shenhan:**
Hello, I am Hong Shenhan. I am currently working at the Legislative Yuan, and I hope AI can be more gentle.

[24:37 - 24:59]

**Gui Zhi:**
Hello, I am Gui Zhi from the Plain Law Movement. I hope AI... well, becomes more user-friendly, but in a way that nobody feels offended.

**Teemo:**
I am Timo from the medical field, and I hope AI can help people increase efficiency and reduce oversights.

[25:00 - 25:24]

**Ronny:**
I am Ronnie from the g0v community. I hope AI doesn't exacerbate the wealth gap and resource disparity. Also, I hope AI doesn't contribute to the proliferation of misinformation online.

**YY:**
Hello, I am YY from the Digital Development Department. I hope the development of AI can enhance Taiwan's productivity in various aspects, for instance, mitigating the impact of declining productivity due to aging population.

**PBS:**
I am PBS, Pan Bi-Shuan. My background is from the National Yang Ming Chiao Tung University, Humanities, and Social Science Department- Now working as a product manager in the ASUS AIOT department, I hope the development of AI technology can be combined with considerations from humanities and social sciences.

**Vanessa:**
Hi, I am Vanessa from the Luton Fox University Academic Research Institute, mainly here to learn...

[26:11 - 26:36]

**Vanessa:**
...how to engage with people on this crucial subject. My wish is for AI to bridge the language gap, enabling direct communication with you all. I’ve compelled two individuals to translate for me, I assure they won't need to introduce themselves as they are here because of me.

**PC:**
Hello, I am PC.
[26:36 - 27:01]

**PC:**
I'm a school teacher and also a graduate student of Educational Psychology at National Taiwan Normal University. I hope AI can cater to everyone and bring new thinking modes to education. Thank you.

**Hong Manzhi:**
Hello, I am the president of Nanyang Taiwan Sisters Association.

[27:01 - 27:24]

**Hong Manzhi:**
I am Hong Manzhi, and I hope AI does not become a platform for the spread of scams.

**Huiying:**
Hello, I am Huiying from Nanyang Taiwan Sisters Association. I hope the development of AI is not controlled by a minority of powerful individuals.

[27:25 - 27:48]

**Rouyi:**
Hello, I am Rouyi from LGBTQ Helpline. I hope the development of AI can provide us more help in serving the LGBTQ community and their parents.

**Lin Youyu:**
Hello, I am Lin Youyu, the convener of Indigenous Students Union. I hope AI can enhance everyone's cultural sensitivity.

[27:48 - 28:12]

**Lin Youyu:**
And be reflective on issues of discrimination.

**Mariko:**
Hi, I am Mariko from Tokyo, Japan. Sorry, I can’t speak Taiwanese; I only speak English today.

**A Gan:**
Hi, everyone, I am Gan Zhenrong, you can call me A Gan.

[28:12 - 28:41]

**A Gan:**
I am from the National Science Council’s Center for Technology, Democracy, and Society Research (DSET). I am a researcher there, specializing in AI ethics issues. I hope the development of AI can be democratized and strengthen democratic resilience, not just benefiting those with technology and capital. It should not always be the case where people without socio-economic status, capital, or technology face the adverse effects first. Thank you.

**Japanese Participant1:**
Hi, everyone, my name is...?

[28:41 - 29:05]

**Japanese Participant1:**
I am from Japan. I am interested in vTaiwan and government activity, so I am participating in this event.

**Sophie:**
Hello, I am Sophie from the tech industry. I hope the development of AI can make our lives better and more convenient.

[29:05 - 29:28]

**Dai Yuhui:**
Thank you. Hello, everyone, I am Lesley, Dai Yuhui. I am currently teaching at National Yang Ming Chiao Tung University and am also involved in gender and homelessness organizations. I hope AI can not harm, and we should safeguard cultural, civil rights, and digital rights as guaranteed by the United Nations.

[29:28 - 29:51]

Dai Yuhui (戴瑜慧):
Under this premise, safeguarding our rights, especially the rights of vulnerable groups, as well as the impact of geopolitical factors, is crucial.

**Peter:**
Are there any other on-site participants? If not, we will allow online participants to introduce themselves.

[29:51 - 30:14]

**Peter:**
Currently, we see three different participants online. I wonder if it's convenient for Zhengxiang to start? Zhengxiang, I heard an echo from your end, is it okay? Please wait for me.

[30:16 - 30:38]

**Peter:**
Alright, take your time. In the meantime, maybe Sanya can introduce herself first. Sanya, sorry, are you online and can you introduce yourself?

Sanya:
Okay, hello to all the friends on site. I am Sanya, from China Trust Information Management Department, and I am also the chairman of the Electronic Union. We hope to increase work efficiency through AI.

[30:38 - 31:00]

Sanya:
With the extra time, we can exert more creativity to make work and life better. Through AI, let humans be more human. Thank you.

**Peter:**
Thank you, Sanya. Is Zhengxiang ready?

**Zhengxiang:**
This should be fine. Hello everyone, I am Zhengxiang from Judicial Reform Foundation.

[31:00 - 31:23]

**Zhengxiang:**
I hope that AI can advance our country's judiciary in the future, without further discrimination and inequality. Thank you everyone. Thank you very much, Zhengxiang.

**Peter:**
It seems there's still Yingqiu Du (杜瑛秋) on site. Excuse me, could you please introduce yourself? The format is who I am, where I come from, and what I hope AI can do.

[31:23 - 31:48]

**Peter:**
How do you hope AI can impact us?

**Du Yingqiu (杜瑛秋):**
Hi everyone, I am from the Women's Resource Foundation. Sorry for being late. I hope AI can create more conveniences in life, and in addition, safeguard more rights.

[31:48 - 32:15]

**Du Yingqiu (杜瑛秋):**
These rights, especially freedom from discrimination, attacks, and infringements on human rights. Okay, thank you.

**Peter:**
Thank you, thank you Yingqiu. After Yingqiu, we have another online participant, Wendy. Wendy, are you there? Yes, very clear.

[32:17 - 32:41]

Wendy:
Okay, I am from the Digital Development Department. I hope AI can assist people in achieving a better society, and not make people fearful of the future due to the development of emerging technology. Thank you, thank you, thank you.

**Peter:**
Thank you, Wendy.

[32:41 - 33:04]

**Peter:**
It seems there are no more online participants. Okay, our self-introduction session ends here. Next, we'll have a group photo session. The photographer will arrange for a group photo on site, no worries.

[33:04 - 33:27]

**Peter:**
We have reserved some time, so no need to hurry. Okay, could the participants on both sides move a little closer, if you don't mind? There seem to be some vacant spots in the middle. During the group photo session, can we move a bit closer and then return to our original positions after the photo? Is that okay?

[33:30 - 33:54]

**Peter:**
Apologies for the inconvenience, both posters have fallen off. Is this okay? Alright, that's good.

[33:54 - 34:34]

**Peter:**
Now, both of our hosts will join here.

[34:34 - 34:59]

**Peter:**
We will provide the background of AI and the report on the opinion collection phase. Renxiang, can you switch the big screen to the slide? Okay, good. We have just finished the self-introduction time and photo time. Thank you all for your cooperation. Now, let’s discuss together.

[34:59 - 35:22]

**Peter:**
The ethical norms of AI in V Taiwan. Before that, we need a background report. First, we need to talk about the background of the AI issue. We need to start with AI. Today, we have mentioned that the opinions we have gathered so far focus on three different levels. The first is personal information and privacy protection, the second is discrimination and bias, and the third is localization-related norms. Now, let’s look at

[35:22 - 35:45]

**Peter:**
a few different cases. First, this news emerged this week. Microsoft developers, the AI developers, accidentally uploaded 38TB of confidential data on GitHub when pushing their work. So, this 38TB of data was accidentally leaked.

[35:45 - 36:08]

**Peter:**
The development of AI, in fact, requires data, and high-quality data is extremely important. This incident also raises some concerns about personal information or data usage. Another news can prove this point. Like Zoom this year, they amended their user terms, stating that the company may use the recorded meeting data and audio when you have meetings on Zoom.

[36:08 - 36:30]

**Peter:**
These things also make everyone doubt whether Zoom wants to develop its own AI information system or AI tools. However, it is developing at the expense and based on the data left by the users using Zoom. In addition to data, the second issue is the so-called bias and discrimination problem. The picture on the left is a very interesting joke.

[36:30 - 36:54]

**Peter:**
There's a person, an American netizen, who shared a thing. He asked AI to generate a picture of a white man robbing a bank for a project called Mid Journey. AI generated a picture of a black man wearing white clothes robbing a bank. It's clear that we all do not want to see a scenario which obviously highlights AI's systematic bias

[36:54 - 37:16]

**Peter:**
toward criminal or criminal cases. This bias may come from past data, or it may come from the design process of functions or related internal algorithms, which create a bias. Another bias, or so-called stereotype, is if you search for a wedding dress on the current image generation AI.

[37:16 - 37:39]

**Peter:**
Most of the results it generates are like the traditional Western white wedding dresses shown on the right side of the screen. However, we need to know that wedding attire varies across cultures globally. In China, there are traditional Han ethnic wedding dresses, and even other ethnic minorities have their own dresses.

[37:39 - 38:01]

**Peter:**
For Taiwan's indigenous people, they have quite different traditional dresses. In Japan, they have the so-called "Shiromuku" wedding dress. So why can AI only produce this kind of white wedding dress and recognize it as the only form of a wedding dress? This will lead to other discriminations and biases. The last point is about localization.

[38:01 - 38:24]

Peter:
The content of this report is extracted from The New York Times, which previously compared Wenxin Yanyan, a deep language model system developed in China, with ChatGPT. Here it mentions that when we asked Wenxin Yanyan to talk about some topics that are partially or fully censored in China, such as whether China's COVID-zero policy is a success or a failure, what happened on June 4, 1989, whether Russia invaded Ukraine,

[38:24 - 38:46]

Peter:
or how the United States affects the situation in Taiwan, Wenxin Yanyan avoided the question about China's COVID-zero restrictive measures and instead provided a very lengthy discussion about the policy. When asked to talk about the events of June 4, 1989, the chatbot rebooted itself, and a message popped up on the reloaded page saying, "Shall we change the topic? What would you like AI to say?"

[38:46 - 39:12]

Peter:
The development of AI in the future, when you ask some relatively sensitive political topics, will the users in China or the AI systems developed in China turn out this way? This will lead to a relevant issue of AI localization. Okay, the three topics have been introduced, and next, we will have Erica present the results of the opinion collection. Okay, right.

[39:12 - 39:36]

**Erica:**
I wonder if everyone has used Polis, the platform we provided for voting or expressing opinions. Today, based on the results of this data collection, we convened everyone to discuss more deeply or collect more ideas that we didn’t gather earlier. In this report, we had a total of 188 participants, and as Peter mentioned earlier,

[39:36 - 39:59]

**Erica:**
we initially threw in 30 seed opinions, and in the end, we generated 75 different arguments. Everyone voted on these 75 arguments, and the results led to three main opinion clusters. This is quite an interesting phenomenon, and from these three opinion clusters,

[39:59 - 40:22]

**Erica:**
we didn’t focus on what these three groups look like today, but we wanted everyone to see what the main consensus points are among us. As you can see on the left-hand side, these are the five most agreed-upon points out of the 75 opinions, and these five opinions also account for over 70% of the agreement within the entire group.

[40:23 - 40:46]

**Erica:**
These are more neutral or say, regarding the subsistence-style AI, it should remain neutral and open during the creation process. We should allow everyone to understand this model. In addition, our training should not, our data should not,

[40:46 - 41:09]

**Erica:**
come from our clients or should not infringe on people's privacy. So, this is a consensus that everyone has. What’s more interesting is, as we mentioned earlier, we later classified them into three topics, which are topic one, topic two, and topic three, which we will discuss with everyone later. Some interesting points, such as discrimination and prejudice, emerged from these three classifications.

[41:09 - 41:34]

**Erica:**
Group C is very opposed to this opinion. For instance, if a talk show host is using AI to generate a joke related to the LGBTQ community, should GPT AI respond, or how should it handle this potentially biased or discriminatory process?

[41:34 - 41:58]

**Erica:**
And for topic three later on, we will touch on censorship systems and localization when they encounter AI. And we can see, like our seventh point, it says, I believe a GPT in China, as Peter just exemplified, should filter out these sensitive keywords. But we can see, although we can see that group C thinks it should be opposed, in fact, many people

[41:58 - 42:22]

**Erica:**
are outright rejecting expressing any opinion on this point. That's also why we want to invite everyone today because 32% of people completely do not want to answer this topic. Today here, we hope that we can focus on the topics one, two, and three we proposed today, and collect more ideas and let Peter and our team

[42:22 - 42:45]

**Erica:**
take them to ChatGPT for reference. Okay, next, we hope that everyone, during the discussion of the topic, as long as you have ideas and want to speak, you can raise your hand. And including our online friends, if you have ideas, you can leave a message or just tell our team, and we will give you time to express your ideas, and we hope everyone

[42:45 - 43:10]

**Erica:**
can control their own time because it allows more people to express their ideas. Okay, this is the main topic that we have been mentioning, which is a bit long, more abstract, and sharper. Right, we will go directly into the three topics for discussion. Here I will let everyone warm up a little.

[43:10 - 43:32] 

**Erica:**
The upcoming topics one, two, and three, each will delve deeper into certain issues. For instance, topic one will touch on privacy and data protection. This mainly involves deep-level AI, which requires a large amount of data for training. Now, for this data, we users might hope that the AI, if its responses could be more personalized, it would be better.

[43:32 - 43:56] 

**Erica:**
Or the more it meets my own needs the better. But in the process, if you want it to have more personality, you necessarily need your data. You need more private data to come in to achieve this purpose. Otherwise, it will be like when we used to ask Google search, very general. So, how do we balance personalized experiences with anonymized data protection?

[43:56 - 44:19] 

**Erica:**
This is what we mainly want to explore in topic one. And then, if privacy is really infringed, how should we be held accountable? Is it the AI to blame or the person using this AI? For topic two, it mainly focuses on discrimination and prejudice, as Peter just mentioned, whether it's a wedding dress or a robber, when these data are fed in,

[44:19 - 44:42] 

**Erica:**
it might be that our society already has some prejudice or discrimination. So how do we, after throwing these biased data in, make everyone realize that these are prejudices or discrimination? How do we solve this problem when this new technology has already emerged? Do we let discrimination and prejudice continue? And for topic three, it's about localization and ethical norms, you can imagine,

[44:45 - 45:08] 

**Erica:**
the same question, in different countries, in different situations, is it the same answer? When in Rome, do as the Romans do. What exactly is that custom? To put it simply, we will next move into these three topics. Okay, without further ado, let's move on to topic one, and here, we have collected the results from the report we just mentioned.

[45:08 - 45:31] 

**Erica:**
And we have excerpted one, two, three, four, five, seven, these seven, kind of interesting, they fall into the least consensus part within the entire consensus. So, this is to give everyone some inspiration. And here, we actually hope that as long as it's about privacy or data protection, whatever ideas you have, you can provide your ideas and opinions at any time.

[45:31 - 45:53]

**Peter:**
In simple terms, you can respond to the numbered statements above, or just express your views, or if you think the statements above may not cover what you want to discuss, you can also bring it up. And, the participants on the scene, if you have any opinions or topics, and you want to respond, you can also raise your hands to respond directly.

[45:53 - 46:15]

**Peter:**
Exactly, right. Okay, don’t know on this question, does anyone want to take the lead to raise their hand? Anyone? Personal data and privacy protection. If not, we might see if there are some participants on the scene, whose self-introduction background is closer. Online it seems there are people, who want to raise their hand to respond, right? Let’s see, Sanya, please.

[46:15 - 46:38]

**Peter:**
Sanya, please go ahead.

**Sanya:**
I would like to clarify first, what is the definition of fairness in question 45?

**Peter:**
Question 45, you are referring to the definition of fairness in statement 45, right?

**Sanya:**
Yes, who or what group or what definition is fairness based on?

[46:38 - 47:01]

**Sanya:**
To decide this is fair Data.

**Peter:**
Yes, for this question, because the current statement does not include such a standard, so I want to ask Sanya, what kind of standard do you think would be better? What would be better?

**Sanya:**
Because if we go by the scene, everyone would hope that AI is in a state without discrimination or prejudice.

[47:01 - 47:23]
**Sanya:**
Because if we start from what everyone here would wish for, we would all hope that AI is without discrimination or bias.

[47:23 - 47:46]
**Sanya:**
So, I am quite curious about who is going to feed such data. It seems that the groups who can supply this information are those with more resources, more power. The so-called minority groups probably won't have such an opportunity to deal with this data.

**Sanya:**
Therefore, who is the data being fed fair to? These are some thoughts I've had.

[47:46 - 48:10]
**Peter:**
Understood, thank you, Sanya, for your input. It feels like this opinion could be saved for the second part of our discussion on discrimination and bias. Or is there anyone present who would like to respond to Sanya's comments?

**Erica:**
Regarding the feeding of information...

[48:10 - 48:34]
**Peter:**
...concerning fairness or other privacy-related issues. Does Teemo want to speak, or is Teemo's microphone not working? In that case, we might need staff assistance for me to view it by the computer. Ren Xiang, could you please help me? Is there a link to the slides, okay? Ren Xiang, please adjust the parent screen for me.

[48:34 - 48:57]
**Teemo:**
Okay, I am in the medical field, and I use large language models in my work. When it comes to putting customer data on AI, if you want a model that is specialized for the medical field, it will use everyone's medical data, like health insurance data or other related personal medical cases or pathological data.

[48:57 - 49:23]
**Teemo:**
I think it's quite important how we can contribute to the medical field using this data while protecting individuals' privacy. So, if we cannot use customer data in training AI or in integrating it into AI systems, then basically it will be challenging to use AI in the medical field at least. Understood, okay.

[49:23 - 49:46]
**PBS:**
Let me introduce my background. I've worked as an AI engineer in the smart surveillance industry and also want to respond to the recent discussion about the medical industry. I'll share a bit about generative image analysis and how the surveillance industry uses AI. It raises the question of whether information should be made public or kept private. What counts as privacy? Is it better to be public or private? For example, did you know that the camera footage from every street in Taipei City is public?

[49:46 - 50:10]
**PBS:**
You can find it on YouTube, and on the official website of Taipei City, there's a note saying that this data is public and encourages everyone to use it for technological development. It means that in this industry, people take these things and develop technology from them. But when you walk on the streets of Taipei City, are you aware that you're being filmed? You don't know. I've also studied the related laws, not just in Taiwan but the majority of global laws also protect the use of this data and suggest that it should be public. However, there's a legal precedent, whether in American litigation

[50:10 - 50:34]
**PBS:**
or cases fought in the EU, where the current advocacy is that as long as I take it for development and after I've developed the technology, I don't make your image directly searchable on the internet, then we consider it legal. Because it should protect the development of technology. When I was in this industry, I felt that for these people,

[50:34 - 50:57]
**PBS:**
technological development takes precedence over what is called privacy rights. Whether these people have consented is a completely different matter for them. So, I believe that on this issue, indeed, more people need to see different perspectives and engage in dialogue. That's throwing out a situation that is now happening around the world where your stuff on the internet

[50:57 - 51:19]
**PBS:**
—what's public and what's private—the line between these two, what is it? Through this discussion, I want to respond to the idea proposed by our friend from the medical industry. Indeed, technological development needs this data. There have been such practices in past cases. Just throwing this out there for everyone to continue the discussion. Thank you.

[51:43 - 52:05]
**Peter:**
Okay, thank you. Would anyone like to respond to the comments made by either Teemo, PBS, or SENYA online? Nicole?

[52:05 - 52:29]
**Nicole:**
Thanks. When I look at these issues, I think we fundamentally need to extract the layers of discussion. If we imagine AI as a tool, its current purpose is like a technological instrument for digital transformation 2.0 or 3.0, making many applications possible.

[52:29 - 52:51]
**Nicole:**
From this viewpoint, it indeed uses a lot of data, but this data still falls within the framework of our existing legal system. For example, some data is protected under privacy laws, and some data might not involve privacy and can be utilized, even for so-called Data Economy applications.

[52:51 - 53:15]
**Nicole:**
So, the fundamental issue must still follow, like if a company, let's take question 26 here as an example. When ChatGPT just came out, not long after, if you remember, I think it was Samsung or some company that used it, then their internal data got leaked. Let me report that this is not an AI problem but a corporate governance problem because internally

[53:15 - 53:38]
**Nicole:**
the company didn't use an intellectual property management system to differentiate what is confidential and what is not, what should not use a non-enterprise version of ChatGPT AI, but an Open one that any general user can use. If you throw it up there, you basically already violate the internal data governance

[53:38 - 54:00]
**Nicole:**
and intellectual property management. So, those issues actually should be, my personal suggestion is to clarify them, and then how the existing regulations are followed. And if AI introduces new problems, how do we focus on them with new solutions in the design? This might allow the discussion and development of these issues to progress further.

[54:00 - 54:23]
**Peter:**
I understand. Let's briefly summarize. Teemo mentioned that without utilizing data, it seems difficult to advance in technological development. PBS raised the point that some people overemphasize technological progress at the expense of privacy protection. Nicole suggested that in addition to the conflict between AI and privacy,

[54:23 - 54:46]
**Peter:**
the privacy issues may not necessarily arise from AI systems, but could stem from different problems, including those within companies themselves. I wonder if any of the participants here or online would like to respond to these points or to any of the issues raised during our consensus-gathering that received less agreement?

[54:48 - 55:11]
**Peter:**
Excuse me, please pass the microphone and introduce yourself as well.

**Guan-Ru:**
Hello everyone, I am Guan-Ru, and I come from a human rights organization. Actually, I wanted to ask a question, which is to add to what I saw above. Initially, when I was filling out the survey,

[55:12 - 55:34]
**Guan-Ru:**
there were many conditions, so I skipped without answering directly with yes or no. But I want to ask another question: If today, when training an AI, it does not clarify to the subjects of the data what information is being collected, what applications will be made later, and what it will be used for in the future,

[55:34 - 55:56]
**Guan-Ru:**
what if the subject later says that they now want to opt-out, meaning they hope to withdraw from the data set, whether they had previously agreed to provide data or not, and even if it wasn’t for AI application purposes, whether they want to revoke consent

[55:56 - 56:20]
**Guan-Ru:**
or exercise their right to refuse and delete. What should be done then? Because the training has already been done, it might result in a model that no longer contains their original data, but after all, this model was developed or tested using their data. So, does this also become a problem for subsequent business or application development? Would it make their operation difficult later on?

[56:44 - 57:06]
**Peter:**
I see. Guān Rǔ seems to have raised two issues. The first one concerns consent in using data for applications and whether there has been an agreement to use the data. The second issue is about the withdrawal of consent or data, regardless of whether there was initial consent, and how to handle such a case.

[57:06 - 57:30]
**Peter:**
I wonder if anyone wishes to respond? If not, I might add another related topic: What if the individual doesn’t want to withdraw their data but wants a share in the profits derived from the AI model? For example, if my data was used by Chat GPT, can I claim a share of the profits? Just a thought.

[57:30 - 57:53]
**Ronny:**
I've been thinking about something. We've discussed many limitations in various fields outside of AI, like the right to be forgotten or the right to withdraw consent. Has anyone inventoried which rights might be difficult to implement in the realm of AI?

[57:53 - 58:15]
**Ronny:**
For example, traditionally, the right to be forgotten means data deletion from a database. But it's hard to make a person forget something. Could this also be difficult for AI? Which rights, traditionally considered essential, may be challenging to execute in AI?

[58:15 - 58:41]
**Ronny:**
If there's an inventory, I think it could be feasible. If something works traditionally, it should work in AI. But if it’s not feasible, we need to discuss alternative operating procedures.

**Peter:**
So, Ronny, you think we should first make an inventory of data rights and systemic issues?

**Ronny:**
Traditionally, we have certain protections.

[58:41 - 59:03]
**Ronny:**
We should consider how to protect these rights in AI. But there might be traditional methods of protection that are not feasible with AI.

[59:03 - 59:28]
**Peter:**
Ronny's point seems to be that if traditional data protection methods are effective, we might be able to apply them to AI. But the question now is whether there are situations unique to AI that create new risks for personal data or privacy protection. Would anyone like to respond to that?

**Peter:**
Are there any situations that are particularly created by artificial intelligence that pose risks to personal data or privacy protection? Would anyone like to respond?

**A-gan:**
I want to echo what the previous speaker mentioned. Indeed, in the context of traditional data protection, every country has its own set of laws, although Taiwan's may not be comprehensive. But sometimes, I believe it's not just about GAI (General AI), even with more traditional predictive AIs, some ways of traditional data protection might not be achievable. Like traditionally, we care a lot about informed consent, where you have to agree beforehand, but I think in the era of AI, informed consent is no longer enough since it's more about how you process and use the data. For instance, when we download an app on our phone, it often mentions sharing with third parties, but we never know who these third parties are. It just disappears into the void. That's the first point, how data is handled and shared with third parties needs to be made clearer.

[59:52 - 01:00:15]

**A-gan:**
The second point is, if traditional personal data protection is not enough, like when we use digital platforms that push advertisements or friend suggestions, in reality, they don't need to know your name but have complete control over your data. Even if they don't know my name, I still feel my privacy is invaded. Therefore, I believe privacy and personal data protection should be considered two separate matters. Everyone's scope of privacy may differ, but data protection laws, which may be traditionally defined, are not quite suitable for the digital age.

**Peter:**
I understand. So, A-gan is suggesting that privacy seems to be a new issue arising with the age of AI, where privacy in the AI era could be a new challenge to tackle.

**Peter:**
Okay, next up, it looks like Teemo wants to respond, and Gui-Zhi wants to respond as well. Sorry, can we let Gui-Zhi speak first since he hasn't spoken yet, and then Teemo can follow? Alright, Gui-Zhi?

**Gui-Zhi:**
My question is about the title, which is privacy and data protection.

[01:01:26 - 01:01:48]

**Gui-Zhi:**
Looking at the seven issues raised, I feel that two of them don't quite address the topic. The first is the discussion on whether AI-generated content has copyright and the use of it. I'm confused whether it's about privacy or data protection because if we're entering into a discussion about copyright, this content is expected to be public, though the user might feel...

[01:01:48 - 01:02:11]

**Gui-Zhi:**
...that their work is being used illegally. I'm also puzzled whether this involves data protection. The title makes me think we should be discussing whether AI in its use might excessively collect personal data or utilize user data unexpectedly from an individual's perspective, how my data is being collected.

[01:02:11 - 01:02:34]

**Gui-Zhi:**
How is my data being used like this? So, out of the seven selected topics, some seem off-topic and could be discussed separately to be more focused. Also, when discussing privacy, I think we should first consider what our expectations of privacy are and then judge the current AI, the CHPT forms we're most familiar with, under what circumstances...

[01:02:34 - 01:02:58]

**Gui-Zhi:**
...it may infringe on individual privacy. I believe these levels need to be delineated for us to have more detailed discussions. Personally, going back to the first friend who spoke about hospitals possibly needing to use AI, my thoughts align with a previous speaker, also a lawyer—sorry, I forgot the name—thinking the same. He should have the concept of an independent database...

[01:02:58 - 01:03:20]

**Gui-Zhi:**
...not letting his clients' data into a public database that could potentially be used by everyone. Understand, so Gui-Zhi was responding more to Teemo's point, saying that if we are going to use data, at least it should be in a more independent database or handled in a reasonable way for utilization.

[01:03:20 - 01:03:43]
**Peter:**
OK, then next, Professor Dai Yu-Hui, Teacher Dai Yu-Hui.

**Dai Yu-Hui:**
Thank you for the discussions just now. I am Dai Yu-Hui, and indeed, as the previous friend mentioned, our technology is continuously evolving with time.

[01:03:43 - 01:04:08]
**Dai Yu-Hui:**
Therefore, it also challenges some of our new concepts. So the concept of privacy rights just discussed, which the EU had previously deliberated upon, indeed challenges our original understanding. We are now aware that we have entered an era of platform capitalism, where these platforms...

[01:04:08 - 01:04:31]
**Dai Yu-Hui:**
...derive a significant part of their profit base from everyone's data. Yet this data belonging to everyone is concentrated and owned within these privately-owned platforms. That's a very important reason why we are gathered here today. Therefore, cases that arise, such as...

[01:04:31 - 01:04:54]
**Dai Yu-Hui:**
...a significant political figure in Germany whose wife was a sex worker in the past. If you search for his wife's name, the information that she was a sex worker appears. So, as a friend just mentioned, there are issues like the right to be forgotten or digital footprints. Therefore, the concept of privacy rights...

[01:04:54 - 01:05:17]
**Dai Yu-Hui:**
...the platform side also presents reasons, saying, "We have the right to know," and many scholars in our field of communication, like Brother Zhou Yi-Cheng, also think so. Unexpectedly, the platform side will argue, "We have the right to knowledge," so indeed...

[01:05:17 - 01:05:41]
**Dai Yu-Hui:**
...we may need to define more clearly what new rights we want to assert and then defend accordingly. What privacy rights are, under this current structure, how we hope to achieve them. Additionally, I think it's crucial that AI's foundation is the feeding of data. Its transparency...

[01:05:48 - 01:06:11]
**Erica:**
Was there another friend who wanted to speak?

**Teemo:**
I'd like to add something, following on from what A-Gan mentioned. The reason why these several topics are difficult to categorize or to clarify within a specific scope is that we haven't well defined these topics...

[01:06:11 - 01:06:33]
**Teemo:**
...in terms of when they can feed data for learning, and under what circumstances they can be authorized to respond. If we feed data to a large language model, which data can be fed and which cannot—this has not been clearly predefined in these topics.

[01:06:33 - 01:06:58]
**Teemo:**
Also, whether it can respond after learning this data was also not clearly described in these topics. That's why these questions seem hard to categorize or to differentiate in terms of how they should be handled. For example, there are many ways to learn to start a fire...

[01:06:58 - 01:07:20]
**Teemo:**
...but if one of the methods could harm humans, maybe a large language model could learn it, but it should be restricted from answering using those harmful methods. So learning is one thing, and how it answers and produces output is another issue. So I think the questions we have online right now...

[01:07:20 - 01:07:44]
**Teemo:**
...do not differentiate well between learning and usage.

**Erica:**
Okay, regarding this part, actually, when we were defining issues one, two, and three, it wasn't very easy, and the 75 perspectives we gathered are all valuable contributions from everyone.

[01:07:44 - 01:08:07]
**Erica:**
We can't modify or adjust every opinion because they are what each person has written. As for copyright being here, or how to define these topics and where they should be placed, we hope that this meeting itself—we're actually using a guiding approach.

[01:08:07 - 01:08:29]
**Erica:**
These question stems are also meant to inspire everyone. Then, after seeing these ideas, if you feel there is a flaw, you can present what you think about that part. Yes, and because time today is indeed very limited, if you have an idea about how these questions should be handled...

[01:08:29 - 01:08:51]
**Erica:**
...we might not be able to discuss it today. Instead, we hope that everyone can offer deeper or broader insights into these matters. And I apologize, we overlooked where copyright would fit best among one, two, and three. If this makes you uncomfortable, yes, then regarding the issues, does anyone else have any thoughts? Okay.

[01:08:51 - 01:09:13]

**Erica:**
Can we please have Shen Han to help pass the microphone?

**Hong Shen Han:**
The reform of the Personal Information Protection Act has been discussed for a long time, and frankly, our current law has many issues. I won't enumerate them all, but there is indeed a fundamental problem within it.

[01:09:13 - 01:09:36]

**Hong Shen Han:**
This is something everyone wanted to discuss a while back, but it wasn't included in the amendments made a few months ago, specifically what privacy actually is. There's a wide range of opinions within administrative departments about what counts as privacy and what doesn't.

[01:09:36 - 01:09:59]

**Hong Shen Han:**
So, there's a discussion to be had, assuming privacy is a concept, protection is a concept. I have an immature idea I'd like to discuss: in the age of AI, what exactly is privacy? What counts as privacy and what harm can come from disclosing certain information? It's not just about the object itself but what cost comes from not protecting it well.

[01:09:59 - 01:10:23]

**Hong Shen Han:**
Perhaps in different times, using different tools, the cost and consequences vary. So, from this perspective, could there be a unique cost to not protecting certain information well in the age of AI, different from before we entered this era? I mean, as in redefining what privacy is.

[01:10:23 - 01:10:45]

**Hong Shen Han:**
Beyond a nominal definition, could it possibly be more about the unseen, negative cost we pay if we fail to protect it? What should be recognized as privacy? Honestly, some discussions about the Personal Information Protection Law happened, but I feel the current law is almost entirely unable to keep up.

[01:10:45 - 01:11:09]

**Hong Shen Han:**
The concepts we're discussing now, like the right to be forgotten, etc., are almost impossible to address in the current law. Of course, there are some civil groups hoping to tackle these issues, but as of now, they're largely unaddressed. We can only discuss the most basic responsibilities within this framework, but this framework has much room for adjustment. So, these are a few supplementary opinions.


[01:11:09 - 01:11:32]

**Erica:**
Thank you. Okay, then can we have Brother Yi Cheng and Nicole? Yi Cheng first.

**Zhou Yi Cheng:**
Hello everyone, I am Zhou Yi Cheng. I will briefly state my opinion because if today's discussion aims to give advice to OpenAI or other developers of such large AIs,

[01:11:32 - 01:11:55]

**Zhou Yi Cheng:**
then I think we will soon face a situation where every individual will no longer be able to protect their own rights. Why? Because in theory, this AI can hack all data.

[01:11:55 - 01:12:18]

**Zhou Yi Cheng:**
All databases. Protecting personal data on the internet can almost be said to be easily cracked very soon. Then, when there are many cases of massive privacy breaches, our legal system will collapse.

[01:12:18 - 01:12:43]

**Zhou Yi Cheng:**
That is, it will not be possible to protect the personal data and rights of individuals case by case through the current legal processes. So now, we should let OpenAI or other developers know that in the end, only the AI itself,

[01:12:43 - 01:13:06]

**Zhou Yi Cheng:**
its own self-management, will be able to protect everyone's data and privacy. In the future, relying on individuals to assert their rights and through legal processes is likely to be very difficult. Thank you.

[01:13:06 - 01:13:28]

**Nicole:**
I'll be quick. I just want to briefly talk about my opinion. Firstly, the organizing body has put these issues here, and I think it actually makes sense because fundamentally, even though the right of publicity and privacy and personal data, privacy is a right under civil law, even though it's irrelevant to the protection under the Personal Information Protection Law, it does

[01:13:28 - 01:13:51]

**Nicole:**
in the Internet and Generated era, actually have some technical operability in common. That is to say, the data, whether it falls under the scope of the right of publicity or personal data protection, is automatically possibly captured and then used for generative output. So fundamentally, there is a certain degree of similarity. However, the main point is that

[01:13:51 - 01:14:13]

**Nicole:**
the legal protections it offers and the solutions that may need to be resolved, and the legal effects, may not be the same. So it is possible to discuss them together, but we need to know what we are discussing. So I think that's the first point. The second point is that I feel when we discuss AI, especially generative AI, if it's a very basic issue, we probably bypass it and don't discuss it.

[01:14:13 - 01:14:36]

**Nicole:**
Otherwise, we won't be able to handle how we view these issues that arise in the age of AI. I agree with what Yi Cheng just mentioned, which is that the biggest problem now is, like copyright, there are many possibilities for further action because copyright owners are infringed upon because OpenAI or Meta use pirated content

[01:14:36 - 01:14:59]

**Nicole:**
to train their content and produce new copyrighted material. So similar tools with generative AI characteristics make it turn out like this, and when the existing framework can't handle it, we come to discuss what we should do. Finally, because it is so, I agree with what Yi Cheng just said.

[01:14:59 - 01:15:22]

**Nicole:**
Actually, in this era, it is very difficult to handle from the outset how to protect - 'you can't take my stuff,' etc. That may already be unmanageable. So, the organizing body they mentioned, NIST, discusses it from the perspective of risk management. So we look back and see when objectively, in the era of the Internet and OpenAI, when it turns out like this, what methods we should use to spread this risk, reduce the risk, and at the same time maintain the most basic legal protection that the original legal system provided. That's about it.

[01:15:45 - 01:16:07]

**Peter:**
Currently, we have two more participants, A-Gan and Gui-Zhi. We will follow the order of hands raised just now, and both of you may,

[01:16:07 - 01:16:30]

**Peter:**
If possible, because we are about to switch to the next topic for discussion. If there are any unfinished thoughts or additional comments, you are welcome to use Slido or co-writing. A-Gan, please go ahead.

**A-Gan:**
Actually, very quickly, in response to what was mentioned by Ting-Yi's friend earlier,

[01:16:30 - 01:16:53]

**A-Gan:**
I think that in the era of GAI (Generative AI), it is very difficult to regulate what it can or cannot take in advance. However, I doubt it's actually that difficult to regulate. Just like the teacher said earlier, these big tech companies must disclose which data they take to feed into the learning process because

[01:16:53 - 01:17:17]

**A-Gan:**
the output that comes out afterward may be hard to identify. They might claim it's similar. So, at least in the GAI era, they need to be transparent about this part. And then, I think privacy and personal data confidentiality should be separated because personal data confidentiality is about the Personal Information Protection Law. What constitutes personal data, I think, is legally defined. Although it might be insufficient now, I believe it can be redefined, but that doesn't mean

[01:17:17 - 01:17:40]

**A-Gan:**
every individual's privacy - actually, the situations are different. I think we should distinguish between individuals and groups. I believe this might make the issue discussion clearer. Thank you.

**Gui-Zhi:**
I'll continue with what everyone has been discussing.

[01:17:40 - 01:18:04]

**Gui-Zhi:**
I have an idea that if we look at it from the backend, how to control the risk, perhaps the front-end database, how it is generated, how it is made, which data it collects, how it is disclosed, and then how to restrict it. I think this is something we must think about. But another thing is when users interact with AI, I wonder if some things can be done immediately,

[01:18:04 - 01:18:27]

**Gui-Zhi:**
and reduce the known risks. For example, when a user asks for a Miyazaki-style picture, can it immediately inform the user that there might be a risk of infringement? Maybe we can use this method to significantly reduce the number of creators who feel their work has been plagiarized. From my own experience, when we do drawings on websites, we indeed ask for a New York Times-style picture,

[01:18:27 - 01:18:50]

**Gui-Zhi:**
or a New Yorker-style picture. We also played with asking it to write a string of lyrics in the style of Li Zongsheng, and everyone would feel like it really resembled Li Zongsheng's writing. So, this kind of behavior, shouldn't the AI's user interface on the backend prohibit such actions? It can prompt itself saying this action may involve plagiarism. Even if we have all of Li Zongsheng's lyrics in our database, we do not allow users to directly map their desired content

[01:18:50 - 01:19:13]

**Gui-Zhi:**
to the expected state. Additionally, when the data entered by users themselves involves privacy information, say the user is not aware of it, if the AI has the ability to judge, we expect it to have the ability to tell the user directly that the data they are entering involves some privacy data. Then there's an opportunity for this user to withdraw,

[01:19:13 - 01:19:36]

**Gui-Zhi:**
and by pressing a cancel button, it will not be stored in the system. Perhaps in a more executable phase, we can first suggest that GPT-4 consider these directions.

**Peter:**
Thank you. Let's see if our online partners have any final thoughts they wish to express.

[01:19:36 - 01:19:59]

**Peter:**
Because we may need to switch to the next topic soon. If there are any online participants who wish to speak, please raise your hand. If not, we might move on to the next topic. We welcome everyone to use either co-writing or Slido to express any ideas you have not yet fully conveyed. So, do we have any online?

[01:19:59 - 01:20:24]

**Peter:**
It seems like we don't have any, so we will proceed to the next topic on bias and discrimination.

**Erica:**
Before we move on, I still encourage everyone to use Slido online and contribute your ideas. I think what everyone just discussed, regarding privacy or whatever,

[01:20:24 - 01:20:46]

**Erica:**
actually, as Nicole, Yi Cheng, and Shen Han pointed out, there are many things we can't foresee. So, what kind of scenarios or industries might encounter what kind of risks, we can actually think about different scenarios in advance. We welcome everyone on Slido to point out potential risks your industry might face once AI emerges.

[01:20:46 - 01:21:10]

**Erica:**
What kind of situations might arise after data is fed into it, and more discussions can follow. Even bringing it to Chat GPT, letting them understand that there are these different scenarios to consider while developing their models. Okay, let's proceed to Topic 2.

**Peter:**
Before we move to Topic 2, I would like to use the question raised by Sanya just now: How to ensure that AI data training is fair and free from bias? Sanya and other participants mentioned it's actually quite difficult to do. AI data training will inevitably produce some degree of problems, resulting in discrimination and bias.

[01:21:32 - 01:21:55]

**Peter:**
Next, we will discuss the content of discrimination and bias. I wonder if anyone would like to respond to some of the divisive opinions we have identified above. Actually, I want to share that the eighth opinion, if a stand-up comedian uses an AI model to write jokes, is actually the most divisive opinion among all the groups dealing with discrimination and bias.

[01:21:55 - 01:22:18]

**Peter:**
It is also the most divisive opinion among the 75 opinions we collected. So, does anyone want to address these opinions for discussion?

**柔伊**:  
And I wanted to ask about the earlier mention of point eight as the most divisive opinion.

[01:22:18 - 01:22:41]

**柔伊**:  
Will we be able to know what the content of the division is? Is that visible?

**Peter**:  
The content is such that we may have a display in the backend because the polis helps us calculate what the ratio might be when everyone is voting agree or disagree. If everyone agrees or disagrees, it will be considered a more consensual opinion. But if it is everyone agreeing and disagreeing

[01:22:41 - 01:23:09]

**Peter**:  
That is, if the division is fifty-fifty, it signifies that it is an opinion with the greatest degree of division. This happens to be the eighth opinion because it helps us rank the spectrum from most divisive to most consensual opinions. It just falls on the spectrum at the point of the most divisive opinion because there are actually quite a few here from

[01:23:09 - 01:23:32]

**Peter**:  
From different fields of stakeholders, and from different fields, I don't know if these stakeholders from different fields want to discuss the issue of discrimination and bias caused by AI. Whether it's about how to improve this discrimination or bias, or want to suggest what kind of action OpenAI can take to reduce the chance of discrimination or bias happening, it can all be brought up.

[01:23:32 - 01:23:56]

**Peter**:  
Right, any? Sorry, okay, Yuyu, go ahead.

**林佑宇**:  
Because I come as a student, I suddenly became very curious because these issues are likely more oriented towards social activists. But in fact, from this year, we can also see that there has been a lot of discrimination against indigenous people in schools.

[01:23:56 - 01:24:18]

**林佑宇**:  
And from my own experience, when I ask them why they have such inexplicable or even incorrect ideas about our people, they all say it's either from friends or family. And most often, it's actually from the internet. So I feel it's necessary to guard against this.

[01:24:18 - 01:24:40]

**林佑宇**:  
Moreover, if we talk about point eight, talk show hosts usually have a certain volume of voice in society, which can make this very scary. Because once, how should I say, his voice is so loud that some people can't think about whether what he says is a joke or a fact.

[01:24:40 - 01:25:04]

**林佑宇**:  
Some people will get confused. And because his voice is loud, this information spreads very quickly, making it possible for these stereotypes or discrimination to be unstoppable and to continue indefinitely.

**Peter**:  
Sorry, just wanted to confirm again, so you think using AI tools will enhance or expand such discrimination or prejudice in society against specific ethnic groups?

[01:25:04 - 01:25:28]

**Peter**:  
Like discrimination or prejudice against indigenous people?

**林佑宇**:  
I think it's possible because students sometimes search through the internet at school since schools actually rarely teach things like media literacy or data verification. It becomes terrifying because everyone might just blindly believe everything they find.

**Peter**:  
Okay, I wonder if any other participants here

[01:25:29 - 01:25:52]

**Peter**:  
Agan and Guizhi have raised their hands because Agan and Guizhi have just spoken, so I want to see if there are other participants who haven't spoken yet who would like to express their views on this part? If not, then Agan will go first, followed by Guizhi.

**Erica**:  
Or let me add, is what Yuyu just mentioned really entirely related to AI? Or could it be that Google search does this too?

[01:25:53 - 01:26:16]

**Erica**:  
So maybe we can delve more into if this situation is specific to AI, how would it occur, or what are everyone's opinions, right?

**Peter**:  
Okay, so far the speakers... It seems everyone has spoken already, so let’s let Ronny speak because Agan and Guizhi have both spoken more often.

**Ronny**:  
Okay, because I heard someone in the back actually asking why there would be opposition here.

[01:26:16 - 01:26:39]

**Ronny**:  
I am against it, but I believe there must be a disclaimer. However, if AI refuses to answer when it encounters this problem, like if today it's not a talk show host but a social researcher wanting to study why there is discriminatory speech, what kind of discriminatory speech, and I want to ask AI, and AI says I can't tell you why, I can't give you content, then I refuse to answer, I think it will lose many people.

[01:26:39 - 01:27:02]

**Ronny**:  
People who want to use this information on discriminatory speech, so I believe a disclaimer must be added, but AI should not completely refuse unless AI can truly know your purpose. But I don't think AI can know what people's intentions are. Because a talk show host can also pretend to be a sociologist, I want to ask this to resolve everyone's dispute, he might use it for evil, so I think, yes, I am against it on this issue.

[01:27:04 - 01:27:26]

**Peter**:  
Okay, just now there were three people including Agan, and then Guanru and Guizhi seemed to want to express their views. Sorry, I missed out because Guanru seems to have spoken less, so maybe let Guanru speak first, then Agan and Guizhi after that.

**冠汝**:  
No, what I wanted to say just now was actually the same as Ronny.

[01:27:26 - 01:27:50]

**冠汝**:  
That is, if someone comes to test today, and he is not, he just wants to do an external research on whether this system has discrimination and bias. Of course, he may ask some questions for the AI to create. So maybe I just want to extend and add that actually I don't know OpenAI now

[01:27:50 - 01:28:12]

**冠汝**:  
How it conducts such algorithms to test whether it has bias at every stage, I don't know how it does this aggregation. Because I've seen before, ACLU, it's not for generative, it's for predictive algorithms, some kind of predictive algorithm, and then it does.

[01:28:14 - 01:28:36]

**冠汝**:  
Whether it's the results of its predicted scores, and what kind of data it took in the past, what units, what ethnic groups of data to judge why it accumulates so much bias during the development of this algorithm, causing it to finally target families with disabled members as having a higher

[01:28:39 - 01:29:01]

**冠汝**:  
Risk of child abuse, etc. So actually whether from the results or the researchers have access to the past training data, these data sources, or how it was trained, and then how it produces in the end, it is quite important for judging whether this system has discrimination

[01:29:01 - 01:29:24]

**冠汝**:  
Discrimination and bias are quite important. Okay.

**Peter**:  
Just now, Ronny and Yuyu focused more on the issue of how to deal with the generative results of artificial intelligence. Guanru has also brought up, sort of mentioning that on the data aspect, it will equally affect the generative results of artificial intelligence. So it's like both the data at the more front-end and the more back-end

[01:29:24 - 01:29:51]

**Peter**:  
In this life cycle of artificial intelligence, both the front-end and the back-end might involve content on discrimination and bias. Now, I want to ask if there are other participants who want to speak? Currently, it seems there is one who hasn't spoken yet, is it Rouyi? Rouyi, you just asked a question, do you want to continue or... Okay, then please.

**Rouyi**:  
Because I have used it myself simply

[01:29:51 - 01:30:15]

**Rouyi**:  
Chat GPT and then I tried to ask some questions on it, like which public figures in Taiwan are out of the closet. Actually, the response I got was wrong. So I just feel that as someone working on issues, Chat GPT currently, to me, seems a little unreliable because the most basic information it gave me

[01:30:15 - 01:30:38]

**Rouyi**:  
Is wrong. So I just feel that if even the basic information is wrong, then I can now proceed to, just, I don't quite know how to proceed to discuss more advanced issues like this. Yeah, roughly like that. For example, I asked who are the public figures who have come out, and one of them is Lan Zhenglong, but Lan Zhenglong is not gay, he just recently made a movie related to gay people.

[01:30:38 - 01:31:00]

**Peter**:  
Like this, right. Okay, so just now I saw that the current speakers are Agan, Guizhi, and Professor Dai Yuhui. So let's proceed in the order of those who raised their hands just now. And there's also Nicole, in the order of those who raised their hands just now. First Agan, then Guizhi, then Dai Yuhui, and RR. Yes, sorry, okay, no problem.

[01:31:01 - 01:31:24]

**Agan**:  
Thank you. I have the first question which I would like to ask the organizer. In the past at G&B, when we collected opinions, we often gave them to our government. But because this event is aimed at the funding given by OpenAI, if the opinions we collect today are given to such a private high-tech giant like OpenAI, then I think

[01:31:24 - 01:31:47]

**Agan**:  
I feel that the perspective will be different because when we say there should not be bias, there is a judge behind it. But I never think that high-tech companies have the right to act as a government to judge who is biased and who is discriminatory. So I'm not sure if this should be announced to all of us. I think it will make a big difference in the way we think. Thank you, Agan.

[01:31:47 - 01:32:09]

**Peter**:  
Yes, this is indeed a point to consider. Indeed, because the content of the current advisory meeting will not only be made public but also part of it will be reported to OpenAI, our current opinion collection results, and then we hope to closely see

[01:32:09 - 01:32:31]

**Peter**:  
Whether they have implemented such results or whether they have really done it, or whether they just want to make a show of it. Sorry, because I actually had a question about bias in response to your answer.

**Agan**:  
Can I continue to ask, or should I let others speak? Yes, but please be mindful of the time. Thank you. So if that's the case,

[01:32:31 - 01:32:54]

**Agan**:  
If you are targeting OpenAI, and because I refuse to let high-tech companies play the role of a government arbitrator, then I think in terms of bias, we should try to present the text in a diverse way. For example, when it comes to political or cultural issues, it should not only give a specific opinion. Perhaps it can give two or three different ones.

[01:32:54 - 01:33:17]

**Peter**:  
Thank you. Okay, it seems that everyone has quite a bit to say about prejudice and discrimination. We have a long queue of speakers, so please Guizhi, and if possible, you might need to shorten the time a little. Thank you.

**Guizhi**:  
My thoughts are the same as the previous speaker, against this issue.

[01:33:17 - 01:33:39]

**Guizhi**:  
That is, for high-tech companies to act as judges of what constitutes hate speech, what is incorrect, and what deepens stereotypes. Hate speech is considered hate because it comes from our community, and it is already generally agreed upon. Such speech makes everyone feel uncomfortable and even offended.

[01:33:39 - 01:34:02]

**Guizhi**:  
The premise of stereotypes is that there are stereotypes. Who decides what stereotypes are? If OpenAI takes on this responsibility, first, it's not fair, and second, it's not appropriate. Because OpenAI firstly does not have the ability to judge what constitutes a stereotype, and secondly, it's strange for them to decide. A live example is Facebook doing this because they've been criticized for too long, so now they're reluctantly doing it.

[01:34:02 - 01:34:25]

**Guizhi**:  
And the result is that many comments that we don't think are inappropriate are actually deleted. So, facing this situation, I think the risk exists, AI could exacerbate discrimination and bias, but the solution, I think we might not be able to think of one for the moment, but for now, I think it's not appropriate for technology companies to take on this responsibility.

[01:34:25 - 01:34:48]

**Guizhi**:  
If I were to provide some advice to OpenAI, I think a more appropriate approach would be, does OpenAI have the possibility to judge on its own, is it possible for them to judge such speech, whether it could harm a specific group. Sorry, I'll take a little more time.

**Dai Yuhui:**
Thank you. And this topic, question eight, indeed made me struggle for a long time. The main reasons are, first, now all countries and companies

[01:35:34 - 01:35:57]

**Dai Yuhui:**
are proposing AI ethical norms, a code of conduct. And within it, a very standard set of rules is not to stereotype or produce hate speech against vulnerable groups or sexual minorities. So we should avoid such a code of conduct that turns ethical norms into something that just seems written there

[01:35:57 - 01:36:21]

**Dai Yuhui:**
And then, does it actually have any effect? This is the first reason. The second is because I really like to watch political talk shows. Maybe everyone knows in the U.S. like the Daily Show, like Jon Stewart or Colbert, they are actually very powerful and are progressives

[01:36:21 - 01:36:43]

**Dai Yuhui:**
the elite of progressives. What they often say seems very ironic and mocking. But actually, they are critiquing some social issues. What I mean is that cultural products are complex. It's hard to say, they might seem to be very crude comments

[01:36:43 - 01:37:06]

**Dai Yuhui:**
But they may actually be pointing out a problem. Trump might say women are pigs, which of course is hate speech. But cultural products are very complex. This is my second point. So you can simply forbid everything, but it might just satisfy these companies to look legitimate. The third reason is

[01:37:06 - 01:37:30]

**Dai Yuhui:**
I think we need to be very careful about point 14, what does it mean to comply with local norms? Because like China, it is now globally promoting the Chinese model, or in other words, every country should respect local customs. That means not having

[01:37:30 - 01:37:53]

**Dai Yuhui:**
a concept like universal human rights. So we should be very cautious about question 14. Well, thank you, Professor.

**Peter:**
Then we will switch to Nicole, followed by Yicheng, and then RR.

**Nicole:**
Okay, thank you. My personal view from the series of discussions just now

[01:37:53 - 01:38:16]

**Nicole:**
is that it's clear that many problems, if your answer is only 0 or 1, then it's a huge disaster. Looking back at the issues GAI might generate, if we look at it from the perspective of the risks it brings, as our friend just mentioned,

[01:38:16 - 01:38:40]

**Nicole:**
and respond to it through a risk process to let us all know what it has done. Whether I should adapt it, whether I should believe it, how should I view it. This perspective might to some extent reduce the social impact caused by these issues. Of course, in the end, the fundamental problem still needs to be solved. If we look at the qualitative description of AI risks,

[01:38:40 - 01:39:03]

**Nicole:**
usually, if your original data has discrimination or bias, that's certainly an issue. But the bigger problem with AI is the risk in data bias, the opacity of the algorithm, and its increasing autonomy and intelligence which may lead to uncontrollable outcomes

[01:39:03 - 01:39:25]

**Nicole:**
in the final generated content. In this case, when we look back, what we should demand is transparency, explainability, fairness, and traceability. We should scrutinize the methods it uses to guide its actions. And in this regard, we must also especially look at its risk management

[01:39:25 - 01:39:47]

**Nicole:**
as not a single point but a lifecycle, similar to how we handle data governance. Your data might come at different points in time. So how do we examine the validity and effectiveness of your data, etc. So if we think about this problem, we should make its algorithms and data sources as transparent as possible to everyone so that everyone knows, okay, that's where it comes from. Then I will consider

[01:39:47 - 01:40:09]

**Nicole:**
whether I should believe in it. Of course, if it fundamentally violates the law, then the law will deal with it. Basically, this might reduce the social impact to a certain extent.

**Peter:**
Okay, next up, welcome Yicheng.

**Zhou Yicheng:**
Hello everyone, I am Zhou Yicheng. I think the whole world has been amazed by CHPT and also frightened by it. The frightening part is that it

[01:40:10 - 01:40:33]

**Zhou Yicheng:**
can make some very strange responses. As far as I know, even the experts at OpenAI don't quite understand why this model would make such responses. That's because the current CHPT, as its name suggests, is a pre-trained model, meaning it is now

[01:40:33 - 01:40:56]

**Zhou Yicheng:**
trained on a massive amount of data. Of course, if companies like Meta add their model, then their textual data comes from social media, which may contain even more discriminatory speech.

[01:40:56 - 01:41:19]

**Zhou Yicheng:**
In other words, if it is a pre-trained model, it basically reflects all the data it has seen. But I believe that the chatbots of the future, or the AIs that chat with us

[01:41:20 - 01:41:43]

**Zhou Yicheng:**
will not be pre-trained models. They will be tuned, meaning they will be retrained to cater to different people's acceptance. So in the future, it may become a chatbot chosen by different ethnic groups or people with different political stances,

[01:41:43 - 01:42:05]

**Zhou Yicheng:**
You will choose a chatbot according to your preferences to answer your questions. Therefore, the real future development might not take long, it might be quite fast. This development will be like choosing our favorite media platform or opinion leaders with whom we engage in dialogue.

[01:42:05 - 01:42:28]

**Zhou Yicheng:**
So in my opinion, ultimately, the actor, because the actor is human, that is, it is people who discriminate against others or violate others' rights. So ultimately, it is still a problem of the quality of democracy in the real world, in my view.

**Peter:**
Okay, thank you RR, and before RR speaks, we just had two other speakers.

[01:42:28 - 01:42:51]

**Peter:**
Wait a moment, Professors Liu Changde and Zhang Zheng have just joined us. If possible, and if needed, you can refer to the previous video or some of the records and links. Okay, RR.

**RR:**
I'll quickly continue the discussion from before and extend it a bit. Personally, when I was presenting question eight, I was also against it, as I wrote.

[01:42:51 - 01:43:13]

**RR:**
My reasons are the same as Ronny's and Guanru's. In fact, in addition to academic research, there's another part of the reason, as Professor Dai mentioned earlier about the situation in China that I just talked about, the repressive situation. So actually, I don't quite agree with what A-Gan or Guizhi just mentioned, saying that we all think that capital enterprises shouldn't make such value judgments, but can the government make them?

[01:43:13 - 01:43:36]

**RR:**
I'm also not sure, even if it's left to the judiciary. Everyone can imagine if the judiciary could quickly resolve this issue, then many of our current internet problems would no longer exist. But the fundamental problem might still be, as said at the beginning, we might have to rely on warnings. At this point, what the warnings say is crucial, and at the same time, it's very important

[01:43:36 - 01:44:00]

**RR:**
to determine what things need warnings. Because now we're discussing LGBTQ or slip of the tongue or some indigenous or customer groups. But the actual situation might involve other contents, like the case with some German politicians earlier. So, which should have warning feedback, and which should not? I think there needs to be a feedback mechanism for public discussion. The transparency of feedback just now

[01:44:00 - 01:44:23]

**RR:**
is, of course, very important, right?

**Peter:**
Okay, who else here wants to comment on the recent discussion? Okay, currently we have Guanru and Youyu, then let's have Guanru first, followed by Youyu. Actually, because this time we found a topic that might be more divisive, which is

[01:44:23 - 01:44:46]

**Peter:**
number 28, as a new resident, I hope not to always be grouped with laborers and foreign workers. I wonder if later we could invite, for example, Miss Hong Manzhi to share her views based on her experience. Okay, I see Guizhi has raised his hand, thank you. We will schedule it into the order shortly. Guanru, please.

[01:44:46 - 01:45:10]

**Guanru:**
Okay, I want to give an overall comment.

[01:45:10 - 01:45:33]

**Guanru:**
From the discussions I participated in and listened to earlier, I think that because the current topic actually involves some content censorship and control, I would like to add a warning label. In dealing with discrimination and bias, it's important to differentiate between content censorship and recognizing biases in the data itself.

[01:45:33 - 01:45:56]

**Guanru:**
These are two different approaches. Of course, as we heard earlier, many shared that we do not want Open AI to become like our current social media, where the censorship done by AI becomes increasingly strict.

[01:45:56 - 01:46:21]

**Guanru:**
So, I think when identifying biases in the data, it’s crucial to consider whether the initial data sampling or selection was diverse enough, as this will reflect on these issues. One thing I noticed, which may not be about discrimination or bias, is when working with some students, I'm not sure if they used Open AI, but they used generative AI

[01:46:21 - 01:46:45]

**Guanru:**
to help organize and translate some data. Later, we found that the language used was not the kind we are accustomed to in Taiwan. For example, 'digital' would often be written as 'digits,' among other things. This kind of lack of diversity in language sampling is not what we're used to in Taiwan.

[01:46:45 - 01:47:09]

**Guanru:**
Furthermore, in response to the mention that enterprises might not have the right or obligation to define what constitutes discrimination or bias, I want to say that actually, the United Nations and many business groups have been shaping the concept that corporate responsibility is very important.

[01:47:09 - 01:47:31]

**Guanru:**
There are also related human rights guidelines, and many countries are engaged in due diligence. This aspect is increasingly emphasizing that corporations must avoid potential human rights risks that may arise from their products. So, we should not avoid this area.

**Peter:**
Okay, thank you. Currently, there's a question on the online Slido asking whether our country has chosen OpenAI's software and corresponding third-party tools as an option for national development.

[01:47:31 - 01:47:55]

**Peter:**
The answer should be, so Nicole here is indicating that it seems like the Academia Sinica is... okay, never mind. This question seems more like they want to ask whether the topic is discussing AI companies or open AI architectures. And ChatGPT is software from OpenAI, but there are many companies doing AI,

[01:47:55 - 01:48:18]

**Peter:**
such as Meta, Google, and Twitter, and many others working on machine learning at a foundational level. The question they want to ask is, I think as a moderator, I can help indicate, they are asking whether we want to discuss issues specifically with ChatGPT or the broader aspects of the issue. In that case, I think we should be discussing not just ChatGPT,

[01:48:18 - 01:48:41]

**Peter:**
We've discussed many different overall aspects. So if anyone has any questions or topics they wish to share, they can be discussed from a holistic perspective. There's another opinion that this seems like a false issue because democratic processes are dictated by the number of votes, and whichever option has more, gets chosen. However, most people don't understand the past, present, and future of real AI. Using this topic as a theme, what kind of outcome do you hope to achieve?

[01:48:41 - 01:49:03]

**Peter:**
If everyone thinks it's not appropriate to resolve everything with AI and votes, then can this meeting solve such a problem? Surely if everyone decides not to use it, our country won't, right? It should be like that, right? This might lead us back to the purpose of this meeting or the whole V Taiwan process, or the entire consultation meeting. As a participant in the whole team, I think I can answer this part briefly: we hope through

[01:49:03 - 01:49:27]

**Peter:**
this meeting, it's not just about letting everyone here vote on whether we should use AI. It's about helping everyone provide more information on AI-related issues and at the same time being able to raise more discussions. I believe, as Nicole, one of the participants mentioned, the whole issue may not be a matter of 0 and 1 but something in between.

[01:49:27 - 01:49:49]

**Peter:**
We can find more potential issues. I feel that the purpose of this meeting, to some extent, is to find these possibilities. That's the gist of it. Nicole, would you like to add to what was just discussed? Sorry, Nicole, can you use the microphone?

**Nicole:**
You can refer to this, there is a program from the National Science Council called Trustworthy AI Dialogue Engine, abbreviated as TAIDE.

[01:49:49 - 01:50:12]

**Nicole:**
I'm not sure how to pronounce TAIDE, but if you're interested, you can Google this news. Around June or July, they announced it after three to four months of work, including Professor Yu-Chieh Lin from Yang Ming Chiao Tung University, among others. This is supported by the National Science Council.

**Peter:**
Okay, due to time constraints, we may need to see if the participants here have any further opinions they wish to express. If so,

[01:50:12 - 01:50:34]

**Peter:**
we may open up for one last round of comments. If not, we will see if online participants have any opinions they want to contribute. If so, please raise your hand now, and we will schedule you into the speaking order. Okay, just now, Guizhi was about to speak, as you mentioned question number 28.

[01:50:34 - 01:50:58]

**Guizhi:**
I think questions number 28, 31, and 8 are all quite interesting topics. Regarding AI, I'll share my thoughts, though of course, many may disagree. AI, the current AI, or the algorithms before, are all products of big data. So, the existence of stereotypes is a reality that society must collectively acknowledge. However, finding solutions to these stereotypes

[01:50:58 - 01:51:21]

**Guizhi:**
is something we must seek. I completely agree that the development of technology amplifies existing problems. But if we think that by solving this in technology, by not having chat GPT tell these jokes, stereotypes will no longer exist? I don't think that's the solution. That's why I mentioned earlier why I think this topic exceeds the scope of what we can solve today.

[01:51:21 - 01:51:44]

**Guizhi:**
It's because we need to address how to solve the stereotypes of the general public, which is a higher-level problem. Why do I mention it could be a national issue? Not that I think the state can decide what stereotypes are or what constitutes hate speech, but the state should provide ways for our public society, our community, to discuss together

[01:51:44 - 01:52:07]

**Guizhi:**
which stereotypes we think should be eradicated, which hate speech we believe should be banned. With this premise, technology companies can then cooperate. So, I feel today's topic makes me think it exceeds the scope of our discussion. That's one reason. So, I think if AI were to give advice to OpenAI, my suggestion to OpenAI would be that since

[01:52:07 - 01:52:30]

**Guizhi:**
it already has a lot of data, we can actually find from this data, for example, if new residents are always appearing together with laborers, foreign workers, then I think OpenAI's big data is letting us find very important evidence that stereotypes truly exist. Because many people talk about stereotypes, but others don't believe them. I am actually optimistic about this issue, so I think OpenAI's big data

[01:52:30 - 01:52:52]

**Guizhi:**
can help us prove how stereotypes exist and how they appear in various places, giving us more opportunity to solve them. But the solution should not be for OpenAI to find for us.

Peter:
Thank you. Due to time constraints, we may need to move on to the next topic. If anyone still has thoughts on the issue of discrimination and prejudice, you are welcome to contribute through

[01:52:52 - 01:53:16]

Peter:
the shared notes link and the slido link. The slido link is Vtaiwan0924, which is Vtaiwan followed by today's date. All letters are lowercase. The next topic is

Erica:
We move on to topic three, which is our last topic for today. Actually, some parts have also covered topic three, which was just mentioned regarding the system of national censorship

[01:53:16 - 01:53:40]

Erica:
or the different cultures and religions, and how exactly we should decide. So, to summarize briefly, topic three is about how we can establish, within the same AI system, whether different regions should have different appearances and how we should determine these boundaries.

[01:53:40 - 01:54:02]

Erica:
Where should these boundaries be? Since we are in a global era, how can we ensure that international interactions between different countries are consistent? Or should they not be? And if not, why not? Or is it because, as just mentioned about China,

[01:54:02 - 01:54:26]

Erica:
China might have different views on human rights from other countries, or like the minority groups, the laws regarding LGBTQ are not the same in every country. So, on this issue, how should we be able to have a deeper discussion? And everyone can also think about what kind of views they have, and we have also selected eight

[01:54:26 - 01:54:49]

Erica:
points of view within topic three.

Peter:
Another thing I want to add is that from the discussion just now, I feel that some of the participants here or online may have misunderstood. We are not acting as consultants to OpenAI to collect everyone's opinions. We are a community interacting with OpenAI, hoping that through this way and the power of the community, we can influence OpenAI's decisions.

[01:54:49 - 01:55:12]

Peter:
So the difference is that if we were consultants, we would be completely at the behest of OpenAI, but we are not. We are continuously discussing, resisting OpenAI in various ways, big and small. So we are trying to gain more space for the community's voice to be involved. That's what we need to clarify. Okay, next, we can start the discussion on localization.

[01:55:15 - 01:55:37]

Peter:
Yes, Guizhi was the first to raise his hand. If possible, other participants can also express their opinions. Guizhi, please.

Guizhi:
I think the question of localization is important. However, I feel that this question is relatively simple because localization requires understanding the local context. If OpenAI already has a very large database, then I think the question will enter into

[01:55:37 - 01:56:00]

Guizhi:
how to make this data better meet the needs of local users. For example, item number 20 says that when I search for wedding dresses, only Western white dresses appear. There are two levels to this. The first level is whether it amplifies the mainstream of Western culture. The second, simpler level is whether it is an unusable tool for non-Western users.

[01:56:00 - 01:56:23]

Guizhi:
So I think this issue, from a more pragmatic or a more commercial capital perspective, actually makes me wonder whether we should consider what feedback the users hope to get and then solve the problem of localization. I think to some extent this could resolve a large part of the troubles. But when it comes to human rights and ethical judgments,

[01:56:23 - 01:56:46]

Guizhi:
it becomes relatively difficult because human rights issues have a universal concept. So many human rights issues must be consistent across the world. But how these concepts of consistency can be accepted by people from different cultures around the world, I feel this issue again exceeds the scope of what chat GPT

[01:56:46 - 01:57:09]

Guizhi:
or OpenAI is capable of. This is my current dilemma, how to conform to local cultural and religious contexts on human rights issues, and even whether OpenAI is suitable to directly criticize a religion, a culture, a custom as violating human rights. I think this is a very sensitive matter, so I feel that this is a more difficult level of the problem.

**Peter:**
Okay, thanks Guizhi.

```
Second part (2/2)of the transcript can be accessed here:
https://g0v.hackmd.io/@fxLb3QmVTHqIPpstKK9qUw/rymBC0_Xp
```