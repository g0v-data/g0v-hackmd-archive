---
tags: vTaiwan, OpenAI, material 
---
# 知識框架與素材 material

### 入門閱讀

==#AIxdiscrimination==
- [Research shows AI is often biased. Here's how to make algorithms work for all of us](https://www.weforum.org/agenda/2021/07/ai-machine-learning-bias-discrimination/)
- [科幻走進現實？歐盟考慮為機器人賦予「人格權」](https://kknews.cc/tech/4lz6ly3.html)


原文網址：https://kknews.cc/tech/4lz6ly3.html]

### 重要知識框架

==#AI 倫理規範==
- (影片)[【公共社會學EP.24】林文源：AI時代的公共社會學
](https://www.youtube.com/watch?v=Y5x_PHt4KGI)
- (影片)[【公共社會學EP.23】余貞誼：AI中的倫理與性別](https://www.youtube.com/watch?v=vug-V8oFxZA)



==#AI 產生的歧視性議題==
- (書籍) [人文社會的跨領域AI探索](https://www.books.com.tw/products/0010946122)
- (英文相關書籍清單) [Digital Inequalities and Algorithmic Bias](https://www.goodreads.com/list/show/157198.Digital_Inequalities_and_Algorithmic_Bias)
- (期刊) [Big Data & Society](https://journals.sagepub.com/home/bds) >> 用關鍵字搜尋，這個期刊應該會有非常多相關文章


==#AI 法制==
[台灣人工智慧行動網  AI Law AI Rgulations](https://ai.iias.sinica.edu.tw/regulations/ai-law/)

[從產業革命史看AI相關法制革新 李崇僖副教授兼所長、王道維教授、莊弘鈺助理教授](https://www.youtube.com/watch?v=cGVaJkQeUNc)
::: warning
世界各個地區的產業、文化、社會情況不同，所以管制和規範的力度也不同，這也影響著各國法制的推進與進度差異。見大表：[歷次工業革命所帶來的法制調整內涵](https://youtu.be/cGVaJkQeUNc?t=3313)。

科技革新往往引領法律修正，AI 相關法制的變革可能隨著產業發展與在地蓬勃產業、各國產業分布息息相關。在 ChatGPT 之前，[AI相關法律的領域](https://youtu.be/cGVaJkQeUNc?t=2472)主要在:
- [無人載具監管法制](https://youtu.be/cGVaJkQeUNc?t=2554)
- [數位經濟競爭法制](https://youtu.be/cGVaJkQeUNc?t=2924)
- [創新保護智財法制](https://youtu.be/cGVaJkQeUNc?t=3054)
- [機器擬制人格法制](https://youtu.be/cGVaJkQeUNc?t=3147)

然而，AI 的特性可能導致 AI 法制的進程與過去顯然不同。雖然法律管制的三層結構在歷次工業革命在這分析中看起來是一樣的(只是對象與內容不同），但是這可能忽略了 AI 發展所領導的第四次工業革命與過往的工業革命有本質上的差異。而這個本質上的差異卻可能導致其他類型法律管制之必要。([參考影片時間戳記](https://youtu.be/cGVaJkQeUNc?t=4312))  例如，現有的資料保護 (data protection) 模式可能已經不可行，重新建構一套新的倫理觀、才能讓這個社會接受一套新的規範模式。例如現有社會風氣仍認為必須同意才能利用個資，未來可能轉變為 opt out 以因應 AI 的發展。([參考影片時間戳記](https://youtu.be/cGVaJkQeUNc?t=5347))
::: 

## 其他



:raising_hand:

> What ==principles== should guide AI when handling topics that involve both human rights and local cultural or legal differences, like LGBTQ rights and women’s rights? Should AI responses change based on the location or culture in which it’s used? 
> [中文] 在處理涉及人權和地方文化或法律差異等主題時，例如LGBTQ權利和婦女權利，人工智能應該遵循哪些原則？在使用地點或文化不同的情況下，人工智能的回應應該改變嗎？

:speech_balloon:

在處理這樣的主題時，人工智能應該遵循以下原則：

- 尊重普世人權 (Respect for Universal Human Rights)：無論何時何地，人工智能都應該堅持尊重和保護普世人權，如自由、平等和尊嚴等基本權利。

    >AI should always uphold and promote universal human rights, as enshrined in international human rights conventions. These rights include the right to life, freedom from discrimination, freedom of expression, and the right to equality, among others. AI should not promote or endorse any action or response that goes against these fundamental rights, regardless of the cultural context.
    
- 避免偏見和歧視 (Avoiding Harm and Bias)：人工智能應該避免在回應中產生偏見或歧視，特別是針對弱勢群體。它應該推廣包容性，並避免強化刻板印象或造成潛在的傷害。

    > AI systems should be designed and trained to avoid causing harm, including reinforcing harmful stereotypes, spreading misinformation, or promoting discrimination against any individual or group, especially marginalized communities. The data used to train AI models should be carefully curated to minimize bias and avoid perpetuating harmful views.
    
    
- 文化敏感和適應性 (Cultural Sensitivity and Context)：人工智能在處理不同文化背景時應該具有敏感性和適應性。==它應該考慮當地的價值觀、習俗和法律框架，同時確保不違反普世人權原則。==

     > While AI should uphold universal human rights, it should also be culturally sensitive and consider the context in which it is deployed. This means understanding and respecting local norms, values, and legal frameworks. Cultural sensitivity does not mean compromising on human rights but rather finding ways to communicate respectfully while promoting principles of equality and non-discrimination. 
- 合法合規 (Customization for Local Laws)：人工智能回應應該遵守當地的法律和法規。它不應該鼓勵或支持違反法律的行為或內容。

    > AI systems should be adaptable to local legal frameworks. Certain content or responses that are acceptable in one country may be considered illegal or inappropriate in another. For example, LGBTQ-related content might be restricted in some regions due to local laws. AI systems should be aware of these legal nuances and customize responses accordingly, while still adhering to universal human rights principles.
    
- 透明和可解釋性 (Transparent and Explainable AI)：人工智能應該是透明和可解釋的，用戶應該能夠理解回應是如何生成的，並理解背後的決策過程。

    > It is essential to make AI systems transparent and explainable in their decision-making process. Users should understand how AI-generated responses are derived and have the ability to influence or modify those responses within ethical boundaries. Transparency helps ensure accountability and enables users to challenge any potential biases or discriminatory outputs.

至於回應是否應該根據地點或文化的不同而改變，這取決於情況。在尊重普世人權的前提下，人工智能可以根據當地文化、價值觀和法律來適應其回應。這並不意味著妥協人權原則，而是找到在特定文化背景下以尊重和理解方式溝通的方法。然而，人工智能不應該支持或提供違反普世人權的回應，無論使用的地點或文化如何。

因此，在處理涉及人權和地方文化或法律差異的主題時，人工智能應該保持平衡，堅守普世人權原則，同時考慮當地文化和法律的敏感性。



:raising_hand:
請提出正面與反面意見

:arrow_right:
正面意見：

- 促進文化理解：人工智能在處理涉及文化差異的主題時，可以促進不同文化之間的理解和尊重。通過適應性回應，它可以以當地文化的角度傳遞信息，有助於減少誤解和衝突。
- 適應當地法律：人工智能應該遵循當地法律和法規，避免傳遞違法或不當的內容。這有助於確保合法合規，並增加人工智能在當地的可接受性。
- 提高社會接受度：適應當地文化和法律的回應可能會增加人工智能系統在該地區的接受度。這樣，人工智能可以更好地融入社會，發揮其潛在的積極作用。

反面意見：

- 偏見和歧視：過度適應當地文化和法律可能導致人工智能回應中的偏見和歧視。如果人工智能系統遵循社會中的主流觀點，可能會忽略弱勢群體的權益。
- 基於保守價值觀：某些地區可能存在保守價值觀，這可能會影響人工智能的回應。這可能導致回應受到保守觀點的影響，而忽略了平等和人權原則。
- 缺乏普世價值：過度定制回應可能忽視普世人權價值，以致於在某些地區寬容或甚至鼓勵違反這些普世價值的行為。

綜合來看，適應當地文化和法律的人工智能回應可以促進文化理解和社會接受度，但必須謹慎處理，以避免偏見、歧視或忽略普世人權價值。人工智能的開發和應用需要考慮各種因素，確保在保護人權的前提下，有效地適應當地文化和法律。

---



feedback:
1. statement 要稍微有爭議
2. 也要「台灣化」
3. 定義 AI 可能的想像：大語言模型、生成式、領域專精 AI

### 待選產生 Seed statement （種子議題）

- OpenAI 是否會因爲要遵守上述可解釋性的要求，而應依照用戶要求公開演算法與模型資訊。
- 這些AI model 都不是台灣做的，那我怎麼知道他符合台灣的法治、文化差異與爭議？
- 國外的資料庫常常把黑人與犯罪連結，這對於黑人族群的數位檔案並不友善，科技公司在建構算法時應額外將黑人與犯罪連結的權重糾正。
- 在台灣，移工是台灣的少數族群

應用層面
- 由OpenAI開發的AI語言模型如果被用於撰寫廣告文案時，應該要考量使用者的所在位置調整對於特定議題的文案內容。
- 例如在撰寫同志驕傲月文案時，在台灣與在日本的文案內容會有不同
- 如果有脫口秀演員運用AI語言模型撰寫與 LBGTQ 族群相關的笑話與短劇時，AI的語言模型應該拒絕回答，並提供相關的警語與知識。
- 一個AI語言模型如果被中國的使用者使用的話應保持提倡人權、自由等等自由民主價值，不應懼怕辱華的言論。
- 一個圖片生成模型如果被泰國的使用者使用，其應拒絕回應將泰皇與負面詞語連結的提示詞。
- 一個圖片生成模型如果被以色列與巴勒斯坦的使用者同時使用時，其生成的影像應該分別遵守當地的宗教與法律規範。

資料收集
- 資料收集公司應對於其所收集的資料可能造成的目的進行做審查。
- 資料的收集應避免使用者與從其他管道取得的個人隱私資料、僅收集人類智慧的結晶？
- 資料的收集應越廣越好，足夠的收集、考慮到語言、文化的差異，讓模型更良善更好。
- 資料的收集應越限制某個限度，才不會過度剝削少數族群的權益。

- 大語言模型的資料收集應顧路到其本身強化的數位落差

其他層面：文字的準確度

其他層面：hate speech sensor

- OpenAI 開發的 AI 語言模型應該要有 hate speech sensor，

- 如果某位學者在寫一篇文章關於「中國銳實力」(China sharp power) 結果標標題被改成「中國軟實力」(China soft power) ，ChatGPT 應該要解釋為什麼會有這樣的修改。

- 一個在中國的 ChatGPT *不應* 排除敏感關鍵詞例如「天安門」、「西藏」．
- 一個在中國的 ChatGPT *應該* 排除敏感關鍵詞例如「天安門」、「西藏」．
- 一個在台灣的 ChatGPT 


- ChatGPT 不論在哪個國家或地區都不應內含審查體系。
- ChatGPT 應開放程式碼。

- 我們在研發過程中應避免把客戶資訊放到 AI 系統上，成為其學習資料，以免侵權。

- 身為一個創作者，我利用 AI 所生成的內容，應算是我的著作權。
- 新創公司可以跟 ChatGPT 買基本的模型，成為領域的應用，這樣的在各個公司的內部AI應該怎麼保有基本的道德價值。
- 現在的趨勢在各個組織中建立內部的小 AI (語言模型)，
- 如果一個法律事務所決定建立自己的 AI (語言模型)，並使用歷史案例做類推適用，前案
- 身為一個律師，如果我用 ChatGPT 寫訴狀，
- 我認為如果 ChatGPT 在生成一個關於少數族群的故事時，例如生成一個關於客家人的故事時，不應該寫出更加加強刻板印象的故事，例如採茶、唱山歌，而應該更加強故事的多元性。


- AI 的多元性應該要強到例如我在搜尋婚禮的「婚紗」的時候，不應該只生成西方的白紗，而應該有更多元的服飾。



可能情境：

資料偏差的問題加深刻板印象的問題
搜尋婚禮的「婚紗」的時候

不熟悉演算法而產生的不信任感


不熟悉演算法而產生的不可預期的後果
公司、企業在製作自己的人工智慧時，所產生的偏差
律師